{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annopol/entity-recognition/blob/main/Entity_Resolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration of Spark session"
      ],
      "metadata": {
        "id": "EtPHmyKA4uc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nq9BzTIVYPu8"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "RN-qyAmLcrqO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "CW7XErtccuOa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "F54lDeuQdEqT",
        "outputId": "737bb69a-8d57-4a53-fe63-45c55ccdd108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDXPrXsY7DHn",
        "outputId": "93644351-6658-4abe-9b72-670af8ba8887"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yvPZduyHM-n",
        "outputId": "7bf1a5d1-9c82-45c8-c868-5dc4136ccb7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.23.5)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "er36Dgzdd6UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n",
        "\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.2-spark3.2-s_2.12 pyspark-shell\""
      ],
      "metadata": {
        "id": "n8N1-YMtdIdP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "n6Z8hzkMdMdR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "spark = SparkSession.builder.appName('Colab') \\\n",
        "    .config('spark.driver.memory', '16g') \\\n",
        "    .config('spark.executor.memory', '16g') \\\n",
        "    .config('spark.executor.cores', '8') \\\n",
        "    .config('spark.sql.shuffle.partitions', '100') \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "4xkZiFu7z4cj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.newSession()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "qosMLdz6Lmmm",
        "outputId": "e32edee7-e754-4606-e3cc-fe72a56c57dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79d42c2abb20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://41f480723d58:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import of the developed module functions:\n",
        "# %cd /content/gdrive/MyDrive/fiverrentity/data/\n",
        "# import datasets\n",
        "# sys.path.insert(0,'/content/gdrive/MyDrive/fiverrentity/')\n",
        "\n",
        "\n",
        "#Path for the source datasets:\n",
        "# path=\"/content/gdrive/MyDrive/Colab Notebooks/data/\"\n",
        "path=\"/\"\n"
      ],
      "metadata": {
        "id": "jTD3HwxfnZB2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import lit, col , row_number, concat, lower , trim\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "def load_data_mapping(spark: SparkSession, path ) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Load dataset Amzon_GoogleProducts_perfectMapping.csv\n",
        "\n",
        "    Args:\n",
        "        spark: Spark session\n",
        "    Returns:\n",
        "        Spark dataframe for Amazon & Google Product Mapping data\n",
        "    \"\"\"\n",
        "    mapping = spark.read.csv(path+'Amzon_GoogleProducts_perfectMapping.csv', header=True)\n",
        "\n",
        "    w = Window().orderBy(lit('A'))\n",
        "    mapping=mapping.withColumn(\"mapIF\", row_number().over(w))\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "\n",
        "def load_data_amazon(spark: SparkSession, path ) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Load dataset Amazon.csv\n",
        "\n",
        "    Args:\n",
        "        spark: Spark session\n",
        "    Returns:\n",
        "        Spark dataframe for Amazon  data\n",
        "      \"\"\"\n",
        "    amazon = spark.read.csv(path+'Amazon.csv', header=True)\n",
        "    amazon = amazon.withColumn(\"source\", lit(\"Amazon\"))\\\n",
        "            .withColumn(\"description\", trim(lower(\"description\")))\\\n",
        "            .withColumn('title', trim(lower(concat(col('title'), col('manufacturer')))))\\\n",
        "\n",
        "    return amazon\n",
        "\n",
        "\n",
        "def load_data_google(spark: SparkSession, path ) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Load dataset GoogleProducts.csv\n",
        "\n",
        "    Args:\n",
        "        spark: Spark session\n",
        "    Returns:\n",
        "        Spark dataframe for GoogleProducts data\n",
        "    \"\"\"\n",
        "    google = spark.read.csv(path+'GoogleProducts.csv', header=True)\n",
        "    google = google.withColumnRenamed('name','title')\\\n",
        "            .withColumn(\"source\", lit(\"Google\"))\\\n",
        "            .withColumn(\"description\", trim(lower(\"description\")))\\\n",
        "            .withColumn('title', trim(lower(concat(col('title'), col('manufacturer')))))\\\n",
        "\n",
        "    return google"
      ],
      "metadata": {
        "id": "IBnvsai2-T5N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import types as t\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "from pyspark.ml.linalg import DenseVector, SparseVector\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, CountVectorizer, StopWordsRemover, NGram, Normalizer, VectorAssembler, Word2Vec, Word2VecModel, PCA\n",
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.linalg import VectorUDT, Vectors\n",
        "import tensorflow_hub as hub\n",
        "from pyspark.sql.functions import udf\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "\n",
        "def tokenize(df, string_cols):\n",
        "    output = df\n",
        "    for c in string_cols:\n",
        "      output = output.withColumn('temp', f.coalesce(f.col(c), f.lit('')))\n",
        "      tokenizer = RegexTokenizer(inputCol='temp', outputCol=c+\"_tokens\", pattern = \"\\\\W\")\n",
        "        # stands for \"word character\", usually [A-Za-z0-9_]. Notice the inclusion of the underscore and digits.\n",
        "      remover = StopWordsRemover(inputCol=c+\"_tokens\", outputCol=c+\"_swRemoved\")\n",
        "      hashingTF = HashingTF(inputCol=c+\"_swRemoved\", outputCol=c+\"_raw\", numFeatures=40)\n",
        "\n",
        "      idf = IDF(inputCol=c+\"_raw\", outputCol=c+\"_features\")\n",
        "      output = tokenizer.transform(output)\n",
        "      output = remover.transform(output)\n",
        "      output = hashingTF.transform(output)\n",
        "      idfModel = idf.fit(output)\n",
        "      output = idfModel.transform(output)\\\n",
        "          .drop('temp', c+\"_tokens\")\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def top_kw_from_tfidf(vocab, n=3):\n",
        "  @udf(returnType=t.ArrayType(t.StringType()))\n",
        "  def _(arr):\n",
        "    inds = arr.indices\n",
        "    vals = arr.values\n",
        "    top_inds = vals.argsort()[-n:][::-1]\n",
        "    top_keys = inds[top_inds]\n",
        "    output = []\n",
        "\n",
        "    for k in top_keys:\n",
        "      kw = vocab.value[k]\n",
        "      output.append(kw)\n",
        "\n",
        "    return output\n",
        "  return _\n",
        "\n",
        "\n",
        "def tfidf_top_tokens(df, token_cols, min_freq=1):\n",
        "  output = df\n",
        "  for c in token_cols:\n",
        "    pre = c\n",
        "    cv = CountVectorizer(inputCol=pre, outputCol=pre+'_rawFeatures', minDF=min_freq)\n",
        "    idf = IDF(inputCol=pre+\"_rawFeatures\", outputCol=pre+\"_features\", minDocFreq=min_freq)\n",
        "    normalizer = Normalizer(p=2.0, inputCol=pre+\"_features\", outputCol=pre+'_tfidf')\n",
        "    stages = [cv, idf, normalizer]\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    model = pipeline.fit(output)\n",
        "    output = model.transform(output)\\\n",
        "      .drop(pre+'_rawFeatures', pre+'_features')\n",
        "\n",
        "    cvModel = model.stages[0]\n",
        "    vocab = spark.sparkContext.broadcast(cvModel.vocabulary)\n",
        "    output = output.withColumn(pre+'_top_tokens', top_kw_from_tfidf(vocab, n=5)(f.col(pre+\"_tfidf\")))\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "# magic function to load model only once per executor\n",
        "MODEL = None\n",
        "def get_model_magic():\n",
        "  global MODEL\n",
        "  if MODEL is None:\n",
        "      MODEL = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "  return MODEL\n",
        "\n",
        "@udf(returnType=VectorUDT())\n",
        "def encode_sentence(x):\n",
        "  model = get_model_magic()\n",
        "  emb = model([x]).numpy()[0]\n",
        "  return Vectors.dense(emb)\n",
        "\n"
      ],
      "metadata": {
        "id": "MQaY7L5D_j8c"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Datasets"
      ],
      "metadata": {
        "id": "YL15XEnYPwxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mappings = load_data_mapping(spark, path)\n",
        "df1 = load_data_amazon(spark, path)\n",
        "df2 = load_data_google(spark, path)"
      ],
      "metadata": {
        "id": "MkFnHIelZzmK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join both product lists:\n",
        "from pyspark.sql.functions import desc\n",
        "source= df1.union(df2)\n",
        "source = source.limit(2500)\n",
        "test = source.orderBy(desc(\"id\")).limit(1500)"
      ],
      "metadata": {
        "id": "aFcM-eLWYuUB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4IyLyr-48zJ",
        "outputId": "75564aaa-5539-4441-fd5c-166d80562d78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization:"
      ],
      "metadata": {
        "id": "ryPzlxZYP2q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blocking_df= tokenize(source, [\"description\", \"title\"])\n",
        "blocking_df_test = tokenize(source, [\"description\", \"title\"])"
      ],
      "metadata": {
        "id": "pk3vgrr4xNu2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By tokenizing the text in the \"description\" and \"title\" columns of the source DataFrame, you are creating new features that may be useful for downstream tasks such as classification"
      ],
      "metadata": {
        "id": "-9OAO-xbGA5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blocking functions**\n",
        "\n",
        "Definitions for transformation of the text into tokens and their normalization"
      ],
      "metadata": {
        "id": "L-LHSOoYN4-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply normalization to tokens:"
      ],
      "metadata": {
        "id": "dr8V4Hz-OKAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blocking_df = tfidf_top_tokens(blocking_df, [c + '_swRemoved' for c in ['title', 'description']])\n",
        "blocking_df_test = tfidf_top_tokens(blocking_df_test, [c + '_swRemoved' for c in ['title', 'description']])"
      ],
      "metadata": {
        "id": "bLtf-pneOJZU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " these imports are required to perform text preprocessing and feature engineering tasks on a PySpark DataFrame, such as tokenization, stop word removal, and TF-IDF vectorization, to prepare the data for a machine learning model."
      ],
      "metadata": {
        "id": "-_44oz7qpv3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate blocking keys"
      ],
      "metadata": {
        "id": "fB_3p0NeQVM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = ['id','source', 'title', 'description',   'price',\n",
        "              'title_swRemoved', 'description_swRemoved',\n",
        "             'title_swRemoved_tfidf', 'description_swRemoved_tfidf',\n",
        "             'title_encoding', 'description_encoding']\n",
        "LARGEST_BLOCK = 5\n",
        "\n",
        "# from spark_matcher.data import cand_pairs\n",
        "# blok=cand_pairs.generate_pairs(blok, keep_cols, LARGEST_BLOCK)\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "blocking_df = blocking_df.withColumn('title_encoding', encode_sentence(f.coalesce(f.col('title'), f.lit(''))))\\\n",
        "  .withColumn('description_encoding', encode_sentence(f.coalesce(f.col('description'), f.lit(''))))\\\n",
        "  .withColumn('blocking_keys',\n",
        "               f.array_union(f.col('title_swRemoved_top_tokens'), f.array_union(f.col('description_swRemoved_top_tokens'), f.col('title_swRemoved_top_tokens')))\n",
        "             )\n",
        "blocking_df_test = blocking_df_test.withColumn('title_encoding', encode_sentence(f.coalesce(f.col('title'), f.lit(''))))\\\n",
        "  .withColumn('description_encoding', encode_sentence(f.coalesce(f.col('description'), f.lit(''))))\\\n",
        "  .withColumn('blocking_keys',\n",
        "               f.array_union(f.col('title_swRemoved_top_tokens'), f.array_union(f.col('description_swRemoved_top_tokens'), f.col('title_swRemoved_top_tokens')))\n",
        "             )"
      ],
      "metadata": {
        "id": "ijwuGEnTcHAK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is preparing the data for blocking, which is the process of grouping similar records together before applying the matching algorithm. The goal of blocking is to reduce the search space and improve the efficiency of the matching algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Jy9RyXetddD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate candidate pairs"
      ],
      "metadata": {
        "id": "57HGwez8Q7xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = ['id','source', 'title', 'description',  'price',\n",
        "              'title_swRemoved', 'description_swRemoved',\n",
        "             'title_swRemoved_tfidf', 'description_swRemoved_tfidf',\n",
        "             'title_encoding', 'description_encoding']\n",
        "\n",
        "LARGEST_BLOCK = 5\n",
        "\n",
        "node = blocking_df.select(keep_cols)\n",
        "node_test = blocking_df_test.select(keep_cols)\n",
        "keep_pairs = blocking_df.select(f.explode('blocking_keys').alias('blocking_key'), 'id','source')\\\n",
        "  .groupBy('blocking_key')\\\n",
        "  .agg(\n",
        "    f.count('id').alias('block_size'),\n",
        "    f.countDistinct('source').alias('diff_source_cnt'),\n",
        "    f.collect_set('id').alias('id'),\n",
        "  )\\\n",
        "  .filter(f.col('block_size').between(2,LARGEST_BLOCK))\\\n",
        "  .select('blocking_key', f.explode('id').alias('id'))\\\n",
        "  .filter(f.col('diff_source_cnt') >1)\n",
        "\n",
        "left = keep_pairs.withColumnRenamed('id', 'src')\n",
        "right = keep_pairs.withColumnRenamed('id', 'dst')\n",
        "\n",
        "candidate_pairs = left.join(right, ['blocking_key'], 'inner')\\\n",
        "  .filter(f.col('src') != f.col('dst'))\\\n",
        "  .select('src', 'dst')\\\n",
        "  .distinct()\n"
      ],
      "metadata": {
        "id": "hdATq4uYaE3d"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above is creating candidate pairs of listings that may match based on shared blocking keys. The keep_cols variable defines the columns to keep in the output, which include the listing ID, source, title, description, price, title and description with stop words removed, title and description with stop words removed and represented as TF-IDF vectors, and title and description encoded using the Universal Sentence Encoder."
      ],
      "metadata": {
        "id": "p9rMGK4QtwpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_pairs_test = blocking_df_test.select(f.explode('blocking_keys').alias('blocking_key'), 'id','source')\\\n",
        "  .groupBy('blocking_key')\\\n",
        "  .agg(\n",
        "    f.count('id').alias('block_size'),\n",
        "    f.countDistinct('source').alias('diff_source_cnt'),\n",
        "    f.collect_set('id').alias('id'),\n",
        "  )\\\n",
        "  .filter(f.col('block_size').between(2,LARGEST_BLOCK))\\\n",
        "  .select('blocking_key', f.explode('id').alias('id'))\\\n",
        "  .filter(f.col('diff_source_cnt') >1)\n",
        "\n",
        "left_test = keep_pairs_test.withColumnRenamed('id', 'src')\n",
        "right_test = keep_pairs_test.withColumnRenamed('id', 'dst')\n",
        "\n",
        "candidate_pairs_test = left_test.join(right_test, ['blocking_key'], 'inner')\\\n",
        "  .filter(f.col('src') != f.col('dst'))\\\n",
        "  .select('src', 'dst')\\\n",
        "  .distinct()"
      ],
      "metadata": {
        "id": "g8LEiDdS8scl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create graph dataframe**"
      ],
      "metadata": {
        "id": "CGHtRIc3Rcol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes import GraphFrame\n",
        "g = GraphFrame(node, candidate_pairs)\n",
        "g_test=GraphFrame(node_test,candidate_pairs_test)"
      ],
      "metadata": {
        "id": "Wourq0bo94YL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# node.show()"
      ],
      "metadata": {
        "id": "5KRJdho-e5z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions for similarity calculation**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C9RVBNhCRXpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_S__ygyRRXmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "import operator"
      ],
      "metadata": {
        "id": "yoItpstahqjn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@udf(\"double\")\n",
        "def dot(x, y):\n",
        "  if x is not None and y is not None:\n",
        "    return float(x.dot(y))\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def null_safe_levenshtein_sim(c1, c2):\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .otherwise(1 - f.levenshtein(c1, c2) / f.greatest(f.length(c1), f.length(c2)))\n",
        "  return output\n",
        "\n",
        "def null_safe_num_sim(c1, c2):\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .when((f.col(c1) == 0) & (f.col(c2) == 0), 1)\\\n",
        "            .when((f.col(c1) == 0) | (f.col(c2) == 0), 0)\\\n",
        "            .otherwise(1 - f.abs(f.col(c1) - f.col(c2)) / f.greatest(c1, c2))\n",
        "  return output\n",
        "\n",
        "def null_safe_token_overlap(c1, c2):\n",
        "  # is the overlap a significant part of the shorter string\n",
        "  output = f.when(f.col(c1).isNull() | f.col(c2).isNull(), 0)\\\n",
        "            .when((f.size(f.array_distinct(c1)) == 0) | (f.size(f.array_distinct(c2)) == 0), 0)\\\n",
        "            .otherwise(f.size(f.array_intersect(c1, c2)) / f.least(f.size(f.array_distinct(c1)), f.size(f.array_distinct(c1))))\n",
        "  return output\n",
        "\n",
        "def calc_sim(df):\n",
        "  df = df.withColumn('title_lev', null_safe_levenshtein_sim('src.title', 'dst.title'))\\\n",
        "      .withColumn('description_lev', null_safe_levenshtein_sim('src.description', 'dst.description'))\\\n",
        "      .withColumn('title_token_sim', null_safe_token_overlap('src.title_swRemoved', 'dst.title_swRemoved'))\\\n",
        "      .withColumn('description_token_sim', null_safe_token_overlap('src.description_swRemoved', 'dst.description_swRemoved'))\\\n",
        "      .withColumn('price_sim', null_safe_num_sim('src.price', 'dst.price'))\\\n",
        "      .withColumn('title_tfidf_sim', dot(f.col('src.title_swRemoved_tfidf'), f.col('dst.title_swRemoved_tfidf')))\\\n",
        "      .withColumn('description_tfidf_sim', dot(f.col('src.description_swRemoved_tfidf'), f.col('dst.description_swRemoved_tfidf')))\\\n",
        "      .withColumn('title_encoding_sim', dot(f.col('src.title_encoding'), f.col('dst.title_encoding')))\\\n",
        "      .withColumn('description_encoding_sim', dot(f.col('src.description_encoding'), f.col('dst.description_encoding')))\n",
        "\n",
        "  metrics = ['description_lev', 'title_lev', 'price_sim', 'title_tfidf_sim', 'description_tfidf_sim',\n",
        "             'title_encoding_sim', 'description_encoding_sim',\n",
        "             'title_token_sim', 'description_token_sim'\n",
        "            ]\n",
        "\n",
        "  df = df.withColumn('overall_sim', reduce(operator.add, [f.col(c) for c in metrics]) / len(metrics))\n",
        "  return df\n",
        "\n",
        "\n",
        "distance_df = calc_sim(g.triplets)\n",
        "distance_df_test = calc_sim(g_test.triplets)"
      ],
      "metadata": {
        "id": "oSmSuwtLJYZC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distance_df.show()"
      ],
      "metadata": {
        "id": "G2rCCaT7gy8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines several functions for calculating similarity measures between pairs of records, and applies them to a DataFrame containing pairs of records (triplets).The resulting distance_df and distance_df_test DataFrames contain the original triplets, as well as columns for each similarity measure and an overall similarity score. These DataFrames can be used to train and evaluate a matching model.\n"
      ],
      "metadata": {
        "id": "wPHl-ltgw31Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Joining the information from the Perfect Mapping dataset**"
      ],
      "metadata": {
        "id": "DDAvXQNMRmDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "#create training data by joining with perfect matching information:\n",
        "distance_df2=distance_df.withColumn('src_id', f.col('edge.src'))\\\n",
        "             .withColumn('dst_id', f.col('edge.dst'))\\\n",
        "             .withColumn('diff_source', (f.col('src.source') != f.col('dst.source')).cast('integer'))\n",
        "\n",
        "cond=((distance_df2.src_id==mappings.idAmazon) & (distance_df2.dst_id==mappings.idGoogleBase)) |\\\n",
        "       ((distance_df2.src_id==mappings.idGoogleBase) & (distance_df2.dst_id==mappings.idAmazon))\n",
        "\n",
        "distance_df3=distance_df2.join(mappings, on=cond, how='left').distinct()\\\n",
        "          .withColumn('mapped_flg',  f.coalesce((f.col('mapIF')>0).cast('integer'),f.lit(0)))\n",
        ""
      ],
      "metadata": {
        "id": "J-zdEElcUinQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance_df2_test=distance_df_test.withColumn('src_id', f.col('edge.src'))\\\n",
        "             .withColumn('dst_id', f.col('edge.dst'))\\\n",
        "             .withColumn('diff_source', (f.col('src.source') != f.col('dst.source')).cast('integer'))\n",
        "\n",
        "cond=((distance_df2_test.src_id==mappings.idAmazon) & (distance_df2_test.dst_id==mappings.idGoogleBase)) |\\\n",
        "       ((distance_df2_test.src_id==mappings.idGoogleBase) & (distance_df2_test.dst_id==mappings.idAmazon))\n",
        "\n",
        "distance_df3_test=distance_df2_test.join(mappings, on=cond, how='left').distinct()\\\n",
        "          .withColumn('mapped_flg',  f.coalesce((f.col('mapIF')>0).cast('integer'),f.lit(0)))"
      ],
      "metadata": {
        "id": "S2TWF7Wz966q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupBy to check number of matched pairs:\n",
        "prop=distance_df3.groupBy(\"mapped_flg\").count()"
      ],
      "metadata": {
        "id": "0ctYiP8FfGuf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prop.show()"
      ],
      "metadata": {
        "id": "Jb29yc6wQLek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "722f912e-5e17-4f96-ab15-896b05a6946c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-cd4cb1707588>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating dataframe for supervised modeling**"
      ],
      "metadata": {
        "id": "_lWLfJOORyI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['description_lev', 'title_lev', 'price_sim',\n",
        "            'title_tfidf_sim', 'description_tfidf_sim',\n",
        "            'title_token_sim', 'description_token_sim',\n",
        "            'title_encoding_sim', 'description_encoding_sim']\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "df_m = distance_df3\n",
        "df_m_test = distance_df3_test\n",
        "\n",
        "# assemble the vectors to make a final feature set\n",
        "# feature_df = feature_df.withColumn('features', f.array(*[f.col(c) for c in features]))\n",
        "\n",
        "assembler= VectorAssembler(inputCols= ['imputed_{}'.format(item) for item in features],\n",
        "                           outputCol= \"features\",\n",
        "                           handleInvalid=\"keep\")\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "df_m = assembler.transform(df_m)\n",
        "df_m_test = assembler.transform(df_m_test)"
      ],
      "metadata": {
        "id": "w-BlLJspJGlx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCITqTK1A_6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training of the model**"
      ],
      "metadata": {
        "id": "9ZtU3CDtSJkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "# param_grid = {'n_estimators': [50, 75, 100], 'max_depth': [11, 12, 13], 'max_features': ['log2', 'sqrt', None]}\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'mapped_flg', seed=1)\n",
        "rf.setFeaturesCol(\"features\")\n",
        "\n",
        "# Taking 70% of both 0's and 1's into training set\n",
        "train = df_m.sampleBy(\"mapped_flg\", fractions={0: 0.5, 1: 0.8}, seed=10)\n",
        "\n",
        " # Subtracting 'train' from original 'data' to get test set\n",
        "test = df_m_test.sampleBy(\"mapped_flg\", fractions={0: 0.5, 1: 0.8}, seed=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "nCSKa-DD0-WG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "OmWPG6Cgu6bT",
        "outputId": "4cdc8bbe-e466-46e2-a75b-440ff67fac2f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3be3f61c8b67>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfModel = rf.fit(train)\n",
        "predictions = rfModel.transform(df_m_test)\n",
        "#print(\"Training Dataset Count: \" + str(train.count()))\n",
        "#print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "metadata": {
        "id": "WWlfh-Ayu5MS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "d0ecc3da-44b3-4e95-bdca-33cc9d58f20e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3a8b74bf9166>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_m_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(\"Training Dataset Count: \" + str(train.count()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"Test Dataset Count: \" + str(test.count()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.4-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print (predictions.show(1))"
      ],
      "metadata": {
        "id": "3bq1o4IAvbi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = predictions.select(['mapped_flg']).collect()\n",
        "y_pred = predictions.select(['prediction']).collect()\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "1fE_F24xYVVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = cm[0, 0]\n",
        "TN = cm[1, 1]\n",
        "FP = cm[1, 0]\n",
        "FN = cm[0, 1]\n",
        "\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "specificity = TN / (TN + FP)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"F1 Score:\", f1_score)"
      ],
      "metadata": {
        "id": "bs-g8D2zZn0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FeZLFaUydcwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# assuming you have defined the true labels as y_true and predicted probabilities as y_pred\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# plot the ROC curve\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jdf26zB1fBIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfModel.featureImportances"
      ],
      "metadata": {
        "id": "L2-EiY3k8zS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "importances = rfModel.featureImportances\n",
        "\n",
        "# create a list of feature names\n",
        "feature_names = train.columns[:10]\n",
        "print(feature_names)\n",
        "# create a bar chart of feature importances\n",
        "plt.bar(feature_names, importances)\n",
        "\n",
        "# set x-axis label\n",
        "plt.xlabel('Feature')\n",
        "\n",
        "# set y-axis label\n",
        "plt.ylabel('Importance')\n",
        "\n",
        "# set chart title\n",
        "plt.title('Feature Importances')\n",
        "\n",
        "# rotate x-axis labels to improve readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dd3EOUen7Lzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**evaluating the model**"
      ],
      "metadata": {
        "id": "w962VSQMSONr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the evaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"mapped_flg\")\n",
        "# calculate AUC\n",
        "auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
        "print('Test AUC: %0.3f' % auc)\n"
      ],
      "metadata": {
        "id": "cVzqIsUNroli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_auc = evaluator.evaluate(train_predictions, {evaluator.metricName: 'areaUnderROC'})\n",
        "print('Train AUC: %0.3f' % train_auc)"
      ],
      "metadata": {
        "id": "OLqTwdED_qDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Draw ROC curve:\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(rfModel.summary.roc.select('FPR').collect(),\n",
        "         rfModel.summary.roc.select('TPR').collect())\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9BXii8xrscb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AUGqbW-vSTKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate entities:**"
      ],
      "metadata": {
        "id": "UsA2XUeLSRvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Generate entities:\n",
        "predictions_pairs = rfModel.transform(df_m)\n",
        "\n",
        "#Matched entities:\n",
        "generated_entities=predictions_pairs.filter(predictions_pairs.prediction==predictions_pairs.mapped_flg)\n",
        "#apply the model on whole dataset"
      ],
      "metadata": {
        "id": "6safdkW3SpFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_entities.show(10)"
      ],
      "metadata": {
        "id": "Zv3t3HvSm3hJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}